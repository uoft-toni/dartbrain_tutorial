{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial preparations\n",
    "\n",
    "<mark> Both the installation and data download only need to be performed once! </mark>\n",
    "\n",
    "We need to first install all the necessary python packages and download the data we will use in this tutorial. Before we do this, you should create a python environment specifically for processing neuroimaging data with python. [Anaconda](https://www.anaconda.com/products/distribution) is an easy tool to create and manage python environments. Install Anaconda (if don't already have it installed) and create a new environment with the latest version python. Then, with this environment activated (or selected in VS Code), move on to install the necessary python packages.\n",
    "\n",
    "## Python packages\n",
    "All the packages we need to install are included in the requirements.txt file in this repository. We can install these packages with a call to pip (note: in a jupyter notebook, you can make calls to the terminal with a %, this command could also be run within a terminal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the tutorial data\n",
    "\n",
    "We are using data from the Pinel Localizer task which includes a 5-minute functional localizer for a few basic cognitive processes (visual perception, finger tapping, langauge, math). The [original paper](https://bmcneurosci.biomedcentral.com/articles/10.1186/1471-2202-8-91) and [OSF website](https://osf.io/vhtf6/files/) include all the details. The full dataset includes 94 subject and is big (42Gb), we will only grab a subset of the data (3Gb). \n",
    "\n",
    "### Option 1: Direct download\n",
    "The instructions below walk through the process of downloading data using DataLad. The process is sort of complicated and takes a long time. It does demonstrate how to use these important tools for open science, so worth the effort if that's something you care about. Alternatively, if you just want to get the data as quickly as possible, download the data (bundled in a zip file) here:\n",
    "\n",
    "[Direct download of tutorial data](https://www.dropbox.com/s/flfsvgzq3zs6va6/localizer.zip?dl=0)\n",
    "\n",
    "Note: This is *not* the official source of this dataset and provided only as a convenience for this specific set of tutorial scripts.\n",
    "\n",
    "### Option 2: Download through official DataLad instance\n",
    "The data is available through a DataLad instance, which thankfully has a python API. DataLad was included in the requirements.txt file above, so was installed with the rest of the python packages. But, for DataLad to work properly, we also need to install git-annex. See [https://git-annex.branchable.com/install/] for installations instructions (on MacOS, use Homebrew to install: `brew install git-annex`). **Install git-annex before proceeding!**\n",
    "\n",
    "Once git-annex is installed, we can download the data. NOTE: In the code below, make sure to update `localizer_path` to point to a directory on your computer where the data should be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import datalad.api as dl\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# update this path to a local directory on your computer!\n",
    "localizer_path = '/Users/michael/Dropbox/work/data/dartbrains/data/localizer'\n",
    "\n",
    "# clone the datalad repository and create a local dataset instance (this will take several minutes!)\n",
    "dl.clone(source='https://gin.g-node.org/ljchang/Localizer',path=localizer_path)\n",
    "ds = dl.Dataset(localizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloning the dataset to your local computer only provides links to the file structure, we still need to actually download the data. The get calls below will take some time to complete (30-50 mins!), so get it started and go grab a coffee. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the experiment metadata\n",
    "result = ds.get(glob.glob(os.path.join(localizer_path,'*.json')))\n",
    "result = ds.get(glob.glob(os.path.join(localizer_path,'*.tsv')))\n",
    "result = ds.get(glob.glob(os.path.join(localizer_path, 'phenotype')))\n",
    "# download the first 5 subjects fmriprep'd data\n",
    "file_list = glob.glob(os.path.join(localizer_path,'*','fmriprep','sub*'))\n",
    "file_list.sort()\n",
    "for f in file_list[:10]:\n",
    "    result = ds.get(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to note where you installed the tutorial data, you'll need this path for the next scripts in the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e5dab3cc268f0918be187fe9df824823a804de1e6a2bd92411813d121f458aa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
